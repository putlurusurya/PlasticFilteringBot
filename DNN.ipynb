{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z \n",
    "    return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "   \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    dZ = np.array(dA, copy=True)  \n",
    "    dZ[Z <= 0] = 0\n",
    "     \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)           \n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*  np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        \n",
    "        Z = np.dot(W,A_prev)+b\n",
    "        linear_cache = (A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        print(W.shape,A_prev.shape)\n",
    "       \n",
    "    elif activation == \"relu\":\n",
    "       \n",
    "        Z = np.dot(W,A_prev)+b\n",
    "        linear_cache = (A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    del linear_cache\n",
    "    del activation_cache\n",
    "    del Z\n",
    "    #print(cache[1])\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    caches=[]\n",
    "    A_prev=X\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1,L):\n",
    "        A_prev,cache=linear_activation_forward(A_prev,parameters['W'+str(l)],parameters['b'+str(l)],'relu')\n",
    "        caches.append(cache)\n",
    "    AL,cache=linear_activation_forward(A_prev,parameters['W'+str(L)],parameters['b'+str(L)],'sigmoid')\n",
    "    caches.append(cache)\n",
    "    del cache\n",
    "    del A_prev\n",
    "    del L\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    cost = -(1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
    "    cost = np.squeeze(cost)      \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ,A_prev.T)/m\n",
    "    db = np.sum(dZ,axis=1,keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    del A_prev\n",
    "    del W\n",
    "    del b\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db =linear_backward(dZ,linear_cache) \n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    del dZ\n",
    "    del linear_cache\n",
    "    del activation_cache\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] =linear_activation_backward(dAL, current_cache, 'sigmoid')\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, 'relu')\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "    del dA_prev_temp, dW_temp, db_temp\n",
    "    del dAL\n",
    "    del L,m,Y\n",
    "    del current_cache\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters) \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*grads['dW'+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*grads['db'+str(l+1)]\n",
    "    del L\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    for l in range(L):\n",
    "\n",
    "        parameters[\"W\" + str(l+1)] = parameters['W'+str(l+1)]-learning_rate*grads['dW'+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters['b'+str(l+1)]-learning_rate*grads['db'+str(l+1)]\n",
    "    del L  \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "             \n",
    "    m = X.shape[1]                 \n",
    "    mini_batches = []\n",
    "\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        \n",
    "        mini_batch_X = shuffled_X[:,k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:,k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    if m % mini_batch_size != 0:\n",
    "        \n",
    "        mini_batch_X = shuffled_X[:,math.floor(m/mini_batch_size)*mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:,math.floor(m/mini_batch_size)*mini_batch_size:]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    del permutation,m\n",
    "    del mini_batch_X,mini_batch_Y\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        \n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)\n",
    "    del L\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "    for l in range(L):\n",
    "\n",
    "        v[\"dW\" + str(l+1)] = beta*v['dW'+str(l+1)]+(1-beta)*grads['dW'+str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta*v['db'+str(l+1)]+(1-beta)*grads['db'+str(l+1)]\n",
    "       \n",
    "        parameters[\"W\" + str(l+1)] = parameters['W'+str(l+1)]-learning_rate*v['dW'+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters['b'+str(l+1)]-learning_rate*v['db'+str(l+1)]\n",
    "    \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "    \n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)\n",
    "    del L\n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \n",
    "    L = len(parameters) // 2                 \n",
    "    v_corrected = {}                         \n",
    "    s_corrected = {}                         \n",
    "\n",
    "    for l in range(L):\n",
    "\n",
    "        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n",
    "        \n",
    "        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
    "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
    "        \n",
    "        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n",
    "        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n",
    "\n",
    "        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
    "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
    "\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + epsilon)\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)\n",
    "    del L\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "   \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m), dtype = np.int)\n",
    "\n",
    "    a3, caches = forward_propagation(X, parameters)\n",
    "  \n",
    "    for i in range(0, a3.shape[1]):\n",
    "        if a3[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, optimizer, learning_rate = 0.005, mini_batch_size = 64, beta = 0.9,beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 30, print_cost = True):\n",
    "\n",
    "    L = len(layers_dims)             \n",
    "    costs = []                       \n",
    "    t = 0                                                   \n",
    "    m = X.shape[1]                  \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    if optimizer == \"gd\":\n",
    "        pass \n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size)\n",
    "        cost_total = 0\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "            print(a3.shape)\n",
    "            cost_total += compute_cost(a3, minibatch_Y)\n",
    "\n",
    "            grads = backward_propagation(a3, minibatch_Y, caches)\n",
    "\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        cost_avg = cost_total / m\n",
    "        \n",
    "        if print_cost and i % 2 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "        if print_cost and i % 2 == 0:\n",
    "            costs.append(cost_avg)\n",
    "                \n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "plastic=os.path.abspath('plastic')\n",
    "onlyfiles = [ f for f in listdir(plastic) if isfile(join(plastic,f)) ]\n",
    "non_plastic=os.path.abspath('non_plastic')\n",
    "onlyfiles1 = [ f for f in listdir(non_plastic) if isfile(join(non_plastic,f)) ]\n",
    "train_images = np.zeros((len(onlyfiles)+len(onlyfiles1),384,512,3), dtype=np.uint8)\n",
    "train_labels = []\n",
    "for n in range(0, len(onlyfiles)):\n",
    "    train_images[n] = cv2.imread( join(plastic,onlyfiles[n]),1 )\n",
    "    train_labels.append(1)\n",
    "\n",
    "for n in range(0, len(onlyfiles1)):\n",
    "    train_images[len(onlyfiles)+n] = cv2.imread( join(non_plastic,onlyfiles1[n]),1 )\n",
    "    train_labels.append(0)\n",
    "\n",
    "train_labels=[train_labels]\n",
    "train_labels_array=np.asarray(train_labels)\n",
    "#print(train_labels_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 545)\n"
     ]
    }
   ],
   "source": [
    "plastic_test=os.path.abspath('plastic_test')\n",
    "onlyfiles_test = [ f for f in listdir(plastic_test) if isfile(join(plastic_test,f)) ]\n",
    "non_plastic_test=os.path.abspath('non_plastic_test')\n",
    "onlyfiles_test1 = [ f for f in listdir(non_plastic_test) if isfile(join(non_plastic_test,f)) ]\n",
    "test_images = np.zeros((len(onlyfiles_test)+len(onlyfiles_test1),384,512,3), dtype=np.uint8)\n",
    "test_labels = []\n",
    "for n in range(0, len(onlyfiles_test)):\n",
    "    test_images[n] = cv2.imread( join(plastic_test,onlyfiles_test[n]),1 )\n",
    "    test_labels.append(1)\n",
    "\n",
    "for n in range(0, len(onlyfiles_test1)):\n",
    "    test_images[len(onlyfiles_test)+n] = cv2.imread( join(non_plastic_test,onlyfiles_test1[n]),1 )\n",
    "    test_labels.append(0)\n",
    "\n",
    "test_labels=[test_labels]\n",
    "test_labels_array=np.asarray(test_labels)\n",
    "print(train_labels_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(545, 384, 512, 3)\n",
      "(589824, 545)\n"
     ]
    }
   ],
   "source": [
    "train_x_flatten = train_images.reshape(train_images.shape[0],-1).T\n",
    "print(train_images.shape)\n",
    "train_x = train_x_flatten/255\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_flatten = test_images.reshape(test_images.shape[0],-1).T\n",
    "test_x = test_x_flatten/255\n",
    "#print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\putlu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\putlu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in multiply\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\putlu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n",
      "C:\\Users\\putlu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "C:\\Users\\putlu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in multiply\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10) (10, 64)\n",
      "(1, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\putlu\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in less_equal\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 33)\n",
      "(1, 33)\n",
      "Cost after epoch 0: nan\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 33)\n",
      "(1, 33)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 33)\n",
      "(1, 33)\n",
      "Cost after epoch 2: nan\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 33)\n",
      "(1, 33)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 33)\n",
      "(1, 33)\n",
      "Cost after epoch 4: nan\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 33)\n",
      "(1, 33)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 33)\n",
      "(1, 33)\n",
      "Cost after epoch 6: nan\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 64)\n",
      "(1, 64)\n",
      "(1, 10) (10, 33)\n",
      "(1, 33)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10) (10, 545)\n",
      "Accuracy: 0.5761467889908257\n",
      "(1, 10) (10, 295)\n",
      "Accuracy: 0.688135593220339\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [train_x.shape[0], 50, 20, 10,1]\n",
    "parameters = model(train_x, train_labels_array, layers_dims, optimizer = \"adam\",learning_rate = 0.004, num_epochs = 8)\n",
    "\n",
    "predictions = predict(train_x, train_labels_array, parameters)\n",
    "predictions = predict(test_x, test_labels_array, parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
